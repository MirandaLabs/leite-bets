# ğŸ”’ Sistema de RotaÃ§Ã£o de Proxies

## ğŸ“‹ VisÃ£o Geral

Sistema implementado para **rotaÃ§Ã£o automÃ¡tica e aleatÃ³ria de proxies** em todos os scrapers, evitando banimentos e rate limiting das casas de apostas.

## ğŸ¯ Funcionalidades

- âœ… Pool de 10 IPs estÃ¡ticos da Webshare
- âœ… RotaÃ§Ã£o aleatÃ³ria por execuÃ§Ã£o
- âœ… Tracking de qual proxy cada scraper estÃ¡ usando
- âœ… Logs detalhados de uso de proxy
- âœ… Fallback automÃ¡tico caso nÃ£o haja proxies configurados

## ğŸ—ï¸ Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  .env           â”‚
â”‚  IP_1 a IP_10   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ProxyManager    â”‚ â† Gerencia pool e rotaÃ§Ã£o
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  browser.py     â”‚ â† Configura proxy no Playwright
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Scrapers       â”‚ â† Betano, Superbet, EsportesDaSorte
â”‚  (collectors)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## âš™ï¸ ConfiguraÃ§Ã£o

### 1. Adicionar IPs no `.env`

```env
# Proxies Webshare (10 IPs estÃ¡ticos)
IP_1=***
IP_2=***
IP_3=***
IP_4=***
IP_5=***
IP_6=***
IP_7=***
IP_8=***
IP_9=***
IP_10=***

# Se seus proxies precisam autenticaÃ§Ã£o:
# PROXY_USERNAME=seu_usuario
# PROXY_PASSWORD=sua_senha
```

### 2. Configurar Porta/AutenticaÃ§Ã£o (se necessÃ¡rio)

Edite [`scrapers/shared/proxy_manager.py`](scrapers/shared/proxy_manager.py):

```python
return {
    "server": f"http://{proxy_ip}:80",  # â† Ajustar porta
    # Descomentar se precisar autenticaÃ§Ã£o:
    # "username": os.getenv("PROXY_USERNAME"),
    # "password": os.getenv("PROXY_PASSWORD")
}
```

## ğŸš€ Como Funciona

### Exemplo de ExecuÃ§Ã£o

**1Âº Momento:**
```
[BETANO] Usando proxy: ***
[SUPERBET] Usando proxy: ***
[ESPORTESDASORTE] Usando proxy: ***
```

**2Âº Momento (nova execuÃ§Ã£o):**
```
[BETANO] Usando proxy:***
[SUPERBET] Usando proxy: ***
[ESPORTESDASORTE] Usando proxy: ***
```

**A rotaÃ§Ã£o Ã© aleatÃ³ria e automÃ¡tica!** ğŸ”„

### Fluxo de Dados

```python
# 1. Scraper Ã© iniciado
collect()

# 2. Browser Ã© criado com proxy
with sync_playwright() as p:
    browser, context = get_browser_context(p, scraper_name="betano")
    # â†‘ Proxy aleatÃ³rio Ã© selecionado aqui

# 3. ProxyManager escolhe IP aleatÃ³rio
proxy_ip = random.choice([IP_1, IP_2, ..., IP_10])

# 4. Playwright usa o proxy para todas as requisiÃ§Ãµes
page.goto("https://betano.bet.br")  # â† Via proxy
```

## ğŸ“ Arquivos Principais

| Arquivo | FunÃ§Ã£o |
|---------|--------|
| [`scrapers/shared/proxy_manager.py`](scrapers/shared/proxy_manager.py) | Gerenciador central de proxies |
| [`scrapers/shared/browser.py`](scrapers/shared/browser.py) | ConfiguraÃ§Ã£o de browser com proxy |
| [`scrapers/base/browser.py`](scrapers/base/browser.py) | Mesma funÃ§Ã£o (duplicado) |
| [`scrapers/base/betano/collector.py`](scrapers/base/betano/collector.py) | Scraper usando proxy |
| [`scrapers/base/superbet/collector.py`](scrapers/base/superbet/collector.py) | Scraper usando proxy |
| [`scrapers/base/esportesdasorte/collector.py`](scrapers/base/esportesdasorte/collector.py) | Scraper usando proxy |

## ğŸ” Verificar Proxies em Uso

### Via Logs

```bash
docker-compose logs scraper | grep "proxy"
```

SaÃ­da esperada:
```
âœ… 10 proxies carregados: ***, ***, ***...
ğŸ”„ [BETANO] Usando proxy: ***
ğŸ”„ [SUPERBET] Usando proxy: ***
```

### Via CÃ³digo

```python
from scrapers.shared.proxy_manager import proxy_manager

# Ver quantos proxies estÃ£o carregados
print(f"Proxies disponÃ­veis: {proxy_manager.available_proxies_count}")

# Ver qual proxy um scraper estÃ¡ usando
proxy = proxy_manager.get_used_proxy("betano")
print(f"Betano estÃ¡ usando: {proxy}")

# Ver todos os proxies em uso
print(proxy_manager.used_proxies)
# {'betano': '***', 'superbet': '***'}
```

## ğŸ§ª Testar Funcionamento

### Teste 1: Verificar se Proxies EstÃ£o Carregados

```python
from scrapers.shared.proxy_manager import proxy_manager

print(f"Proxies: {proxy_manager.proxies}")
print(f"Total: {proxy_manager.available_proxies_count}")
```

### Teste 2: Simular RotaÃ§Ã£o

```python
from scrapers.shared.proxy_manager import get_random_proxy

# Simular 5 execuÃ§Ãµes
for i in range(5):
    betano_proxy = get_random_proxy("betano")
    superbet_proxy = get_random_proxy("superbet")
    print(f"ExecuÃ§Ã£o {i+1}:")
    print(f"  Betano: {betano_proxy}")
    print(f"  Superbet: {superbet_proxy}")
    print()
```

### Teste 3: Rodar Scraper Completo

```bash
# Via Docker
docker-compose up scraper

# Ou localmente
python scrapers/workflows/run_betano.py
```

## âš ï¸ Troubleshooting

### Problema: Nenhum proxy carregado

**Sintoma:**
```
âš ï¸  Nenhum proxy encontrado no .env - scrapers rodarÃ£o sem proxy
```

**SoluÃ§Ã£o:**
1. Verificar se `.env` tem os IPs configurados
2. Verificar se estÃ¡ carregando o `.env` coreto (no Docker, adicionar ao docker-compose.yml)

### Problema: Proxies nÃ£o funcionam

**Sintoma:** Timeout ou erro de conexÃ£o

**SoluÃ§Ãµes:**
1. **Verificar porta:** Ajustar em `proxy_manager.py` (linha ~66)
   ```python
   "server": f"http://{proxy_ip}:80"  # Trocar 80 pela porta correta
   ```

2. **Adicionar autenticaÃ§Ã£o:** Se seus proxies precisam user/pass
   ```python
   "username": os.getenv("PROXY_USERNAME"),
   "password": os.getenv("PROXY_PASSWORD")
   ```

3. **Testar proxy manualmente:**
   ```bash
   curl -x http://***:80 https://api.ipify.org
   ```

### Problema: Sempre usa o mesmo proxy

**Causa:** Seed do random pode estar fixo

**SoluÃ§Ã£o:** JÃ¡ estÃ¡ implementado com `random.choice()` - cada execuÃ§Ã£o deve ser diferente

## ğŸ“Š EstatÃ­sticas de Uso

Para adicionar mÃ©tricas de uso dos proxies:

```python
# Em proxy_manager.py, adicionar:
class ProxyManager:
    def __init__(self):
        self.proxies = self._load_proxies()
        self.used_proxies = {}
        self.usage_stats = {}  # Novo
    
    def get_random_proxy(self, scraper_name: str = None):
        proxy_ip = random.choice(self.proxies)
        
        # Incrementar contador
        self.usage_stats[proxy_ip] = self.usage_stats.get(proxy_ip, 0) + 1
        
        return proxy_ip
    
    def get_stats(self):
        """Retorna estatÃ­sticas de uso"""
        return self.usage_stats
```

## ğŸ” SeguranÃ§a

### Boas PrÃ¡ticas

âœ… **NÃ£o commitar** o `.env` com IPs reais  
âœ… **Rotacionar** IPs regularmente (mensalmente)  
âœ… **Monitorar** se algum IP estÃ¡ bloqueado  
âœ… **Ter backup** de IPs extras caso algum falhe  

### Adicionar ao `.gitignore`

```gitignore
.env
.env.local
*.env.backup
```

## ğŸš€ PrÃ³ximos Passos (Melhorias Futuras)

- [ ] Sistema de health check para testar proxies periodicamente
- [ ] Remover automaticamente proxies que falharem
- [ ] Load balancing inteligente (menos uso em proxies mais usados)
- [ ] MÃ©tricas de performance por proxy
- [ ] RotaÃ§Ã£o baseada em tempo (trocar proxy a cada X minutos)
- [ ] Suporte a diferentes tipos de proxy (SOCKS5, HTTPS)

## ğŸ“ Changelog

**v1.0.0** - Sistema de Proxies
- âœ¨ ImplementaÃ§Ã£o inicial do ProxyManager
- âœ¨ RotaÃ§Ã£o aleatÃ³ria de 10 IPs
- âœ¨ IntegraÃ§Ã£o com todos os scrapers
- âœ¨ Logs detalhados de uso
- âœ¨ Fallback automÃ¡tico sem proxies

---

ğŸ’¡ **Dica:** Configure mais IPs no `.env` (IP_11, IP_12, etc.) para aumentar o pool. O sistema detecta automaticamente!
